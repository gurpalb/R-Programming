# Pseudo R2
1 - (logLik(z.null)/logLik(z1))^(2/nrow(mydata))
#Scaled R2
(1 - (logLik(z.null)/logLik(z1))^(2/nrow(mydata)))/(1-logLik(z.null)^(2/nrow(mydata)))
(1 - (logLik(z.null)/logLik(z1))^(2/nrow(mydata)))/(1 - (-13.86294^(2/nrow(mydata))))
1 - (logLik(z.null)/logLik(z1))^(2/nrow(mydata))
# Notes:
predict(z1) # will give predicted values on the logit scale
fitted(z1) # will give predicted values on the original scale (as predicted probabilities)
predict(z1, type="response") # will also give predicted values on the original scale
# Create new data so that you can plot using it
xnew <- seq(min(mydata$age), max(mydata$age), length.out = 100)
xnew
ynew <- predict(z1, data.frame(age = xnew), type="response")
ynew
# PLOT WITH MODEL FIT AND PREDICTED VALUES
plot(purchase ~ age, data=mydata, pch=16)
lines(xnew, ynew, lty=1)
points(mydata$age, predict(z1, type="response"))
# Add predicted values to a column in a new dataset (so that you are not altering your original dataset)
predict1 <- predict(z1, type="response")
mydata.2 <- cbind(mydata, predict1)
mydata.2
# Subset the data into purchase and no purchase
mydata.2.purchase <- subset(mydata.2, mydata.2$purchase == 1)
mydata.2.no.purchase <- subset(mydata.2, mydata.2$purchase == 0)
# Plot the data and model fit with color coding
plot(purchase ~ age, data=mydata, pch=1, col = c("red", "blue")[as.factor(mydata$purchase)])
lines(xnew, ynew, lty=1)
points(mydata.2.purchase$age, mydata.2.purchase$predict1, col="blue", pch=16)
points(mydata.2.no.purchase$age, mydata.2.no.purchase$predict1, col="red", pch=16)
abline(0.5, 0, lty=2) # This is the line for the mean of the population, where we might want to put our cut-off
# Compare the results in this table to the results that we can see on the graph
table(mydata$purchase, fitted(z1) > 0.5)
# Let's try some other cut-off points to see what happens to our results
# cut-off at 0.2
plot(purchase ~ age, data=mydata, pch=1, col = c("red", "blue")[as.factor(mydata$purchase)])
lines(xnew, ynew, lty=1)
points(mydata.2.purchase$age, mydata.2.purchase$predict1, col="blue", pch=16)
points(mydata.2.no.purchase$age, mydata.2.no.purchase$predict1, col="red", pch=16)
abline(0.2, 0, lty=2)
table(mydata$purchase, fitted(z1) > 0.2)
# cut-off at 0.6
plot(purchase ~ age, data=mydata, pch=1, col = c("red", "blue")[as.factor(mydata$purchase)])
lines(xnew, ynew, lty=1)
points(mydata.2.purchase$age, mydata.2.purchase$predict1, col="blue", pch=16)
points(mydata.2.no.purchase$age, mydata.2.no.purchase$predict1, col="red", pch=16)
abline(0.6, 0, lty=2)
table(mydata$purchase, fitted(z1) > 0.6)
# This will try out many different cut-off points to give you an idea of how to maximize or minimize different values.
# For example, you might want to maximize percentage of correct predictions.
# For example, you might want to minimize false negatives.
# Create an empty dataframe that you will fill with
df <- data.frame(matrix(ncol = 9, nrow = 51))
colnames(df) <- c("correct.event", "correct.non.event", "incorrect.event", "incorrect.non.event", "correct.percent", "sensitivity", "specificity", "false.pos", "false.neg")
df
prob.level <- seq(0, 1, length.out=51) # create a vector with different possible probabilities
prob.level
class.table.data <- cbind(prob.level, df) # combine your vector of probabilities and your empty dataframe
class.table.data # Your dataframe has one row for each probability cut-off
# fill empty cells in your dataframe with 0
class.table.data$correct.non.event <- rep(c(0), c(51))
class.table.data$correct.event <- rep(c(0), c(51))
class.table.data$incorrect.non.event <- rep(c(0), c(51))
class.table.data$incorrect.event <- rep(c(0), c(51))
class.table.data
# This loop will try out the different probability cut-off values and fill in how many correct and incorrect events and non-events you have based on your data.
for (i in 1:51) {
class.table <- table(mydata$purchase, fitted(z1) > class.table.data$prob.level[i])
col.true.num <- grep("TRUE", colnames(class.table))
col.false.num <- grep("FALSE", colnames(class.table))
if (length(col.true.num) > 0) {
class.table.data$incorrect.non.event [i] <- class.table[1, col.true.num]
class.table.data$correct.event [i] <- class.table[2, col.true.num] }
if (length(col.false.num) > 0) {
class.table.data$correct.non.event [i] <- class.table[1, col.false.num]
class.table.data$incorrect.event [i] <- class.table[2, col.false.num] }  }
class.table.data
# You will use this information to fill in the rest of your classification table.
class.table.data$correct.percent <- (class.table.data$correct.event + class.table.data$correct.non.event)/nrow(mydata)
class.table.data$sensitivity <- (class.table.data$correct.event)/nrow(mydata)
class.table.data$specificity <- (class.table.data$correct.non.event)/nrow(mydata)
class.table.data$false.neg <- (class.table.data$incorrect.non.event)/nrow(mydata)
class.table.data$false.pos <- (class.table.data$incorrect.event)/nrow(mydata)
class.table.data
xnew <- seq(min(mydata$age), max(mydata$age), length.out = 100)
xnew
ynew <- predict(z1, data.frame(age = xnew), type="response")
ynew
#zhat <- predict(z1, se.fit=TRUE)
zhat <- predict(z1, data.frame(age = xnew), se.fit=TRUE)  # result on logit or log scale
zupper <- zhat$fit + 1.96 * zhat$se.fit
zlower <- zhat$fit - 1.96 * zhat$se.fit
yupper <- exp(zupper)/(1 + exp(zupper)) # for logit link, backtransform to untis of response variable
ylower <- exp(zlower)/(1 + exp(zlower))
plot(purchase ~ age, data=mydata, pch=1, col = c("red", "blue")[as.factor(mydata$purchase)])
lines(xnew, ynew, lty=1)
points(mydata.2.purchase$age, mydata.2.purchase$predict1, col="blue", pch=16)
points(mydata.2.no.purchase$age, mydata.2.no.purchase$predict1, col="red", pch=16)
lines(xnew, yupper, lty=2)
lines(xnew, ylower, lty=2)
# read in the csv file
mydata <- read.csv("KidCreative dataset.csv", header=TRUE)
str(mydata)
View(mydata)
# read in the csv file
mydata <- read.csv("KidCreative dataset.csv", header=TRUE)
str(mydata)
plot(buy ~ income, data=mydata, pch=16)
plot(buy ~ income, data=mydata, pch=16, col = "blue", main = "Buy Vs. Income", xlab = "Income ($/year)", ylab = "Buy")
lines(lowess(mydata$buy, mydata$income))
lines(lowess(mydata$buy, mydata$income))
# Lowess curve
lines(lowess(mydata$buy, mydata$income, delta=0.1), col="red")
# Lowess curve
lines(lowess(mydata$income, mydata$buy, delta=0.1), col="red")
z1 <- glm(buy, data=mydata, family="binomial"(lin ="logit"))
summary(z1)
z.null <- glm(purchase ~ 1, data=mydata, family="binomial"(link="logit"))
summary(z.null)
z.null <- glm(buy ~ 1, data=mydata, family="binomial"(link="logit"))
summary(z.null)
exp(-1.47796)/(1 + exp(-1.47796))
mean(fitted(z1))
mean(mydata$buy)
logLik(z.null)
### 6.	Fit a model with income only. Use R to obtain the log likelihood for this model.
###############################
# INTERPRETING CO-EFFICIENTS
###############################
# Fit the Continuous-only model
# glm = generalized linear model
# link says values can only take on 0 or 1
z1 <- glm(buy ~ income, data=mydata, family="binomial"(link="logit"))
summary(z1)
logLik(z1)
# You can compare the null model and your model using a likelihood ratio test
anova(z.null, z1, test="Chi")
-2*(logLik(z.null) - logLik(z1))
confint(z1)
install.packages("AICcmodavg")
# Double check with codes
library(AICcmodavg)
# AIC, smaller (more negative) is better
AIC(z1)     # takes into account number of variables (penalized if more)
# [1] 23.05908
AICc(z1)    # takes into account number of variables and sample size
# [1] 23.76496
AIC = -2*ln(396.7289)+2*(1+1)
AIC = -2*log(logLik(z1))+2*(1+1)
AIC = -2*(logLik(z1))+2*(1+1)
AIC
-2*(logLik(z1))+2*(1+1)
# Calculate AIC
-2*(logLik(z1))+2*(1+1)
AIC(z1)     # takes into account number of variables (penalized if more)
AICc(z1)    # takes into account number of variables and sample size
z2 <- glm(buy ~ married, data=mydata, family="binomial"(link="logit"))
summary(z2)
z2 <- glm(buy ~ Married, data=mydata, family="binomial"(link="logit"))
summary(z2)
logLik(z2)
z2 <- glm(buy ~ as.factor(Married), data=mydata, family="binomial"(link="logit"))
summary(z2)
# Coefficients:
#     Estimate Std. Error z value Pr(>|z|)
# (Intercept) -9.344e+00  8.315e-01  -11.24   <2e-16 ***
#    income       1.494e-04  1.336e-05   11.18   <2e-16 ***
logLik(z2)
as.factor(mydata$Married)
str(mydata)
mydata$Married <- factor(mydata$Married)
str(mydata)
z2 <- glm(buy ~ Married, data=mydata, family="binomial"(link="logit"))
summary(z2)
logLik(z2)
# You can compare the null model and your model using a likelihood ratio test
anova(z.null, z2, test="Chi")
-2*(logLik(z.null) - logLik(z2))
-2*(logLik(z2))+2*(1+1)
AIC(z1)     # takes into account number of variables (penalized if more)
AICc(z1)    # takes into account number of variables and sample size
-2*(logLik(z2))+2*(1+1)
AIC(z2)     # takes into account number of variables (penalized if more)
AICc(z2)    # takes into account number of variables and sample size
plot(jitter(buy, f = 0.5)~ income, data=mydata, pch=16)
lines(lowess(mydata$income, mydata$buy, delta=0.1), col="red")
str(mydata)
mydata$Female <- factor(mydata$Female)              # Female
mydata$College <- factor(mydata$College)            # College
mydata$Professional <- factor(mydata$Professional)  # Professional
mydata$Retired <- factor(mydata$Retired)            # Retired
mydata$Unemployed <- factor(mydata$Unemployed)      # Unemployed
mydata$Dual.Income <- factor(mydata$Dual.Income)    # Dual.Income
mydata$Children <- factor(mydata$Children)          # Children
mydata$Prev.Child.Mag <- factor(mydata$Prev.Child.Mag)      # Prev.Child.Mag
mydata$Prev.Parent.Mag <- factor(mydata$Prev.Parent.Mag)    # Prev.Parent.Mag
str(mydata)
z3 <- glm(buy ~ Female, data=mydata, family="binomial"(link="logit"))
AIC(z3)                             # AIC value
-2*(logLik(z.null) - logLik(z3))    # G value
summary(z3)                         # p value
anova(z.null, z3, test="Chi")       # p value
z4 <- glm(buy ~ College, data=mydata, family="binomial"(link="logit"))
AIC(z4)                             # AIC value
-2*(logLik(z.null) - logLik(z4))    # G value
anova(z.null, z4, test="Chi")       # p value
plot(jitter(buy, f = 0.5)~ income, data=mydata, pch=16, col = c("red","blue")[mydata$married]
plot(jitter(buy, f = 0.5)~ income, data=mydata, pch=16, col = c("red","blue")[mydata$married])
plot(jitter(buy, f = 0.5)~ income, data=mydata, pch=16, col = c("red","blue")[mydata$married])
plot(jitter(buy, f = 0.5)~ income, data=mydata, pch=16, col = c("red","blue")[(mydata$married])
plot(jitter(buy, f = 0.5)~ income, data=mydata, pch=16, col = c("red","blue")[as.Factor(mydata$married])
plot(jitter(buy, f = 0.5)~ income, data=mydata, pch=16, col = c("red","blue")[as.factor(mydata$married])
plot(jitter(buy, f = 0.5)~ income, data=mydata, pch=16, col = c("red","blue")[as.factor(mydata$married]))
plot(jitter(buy, f = 0.5)~ income, data=mydata, pch=16, col = c("red","blue")[as.factor(mydata$married)])
plot(buy ~ income, data=mydata, pch=16, col = "blue", main = "Buy Vs. Income", xlab = "Income ($/year)", ylab = "Buy")
plot(jitter(buy, f = 0.5)~ income, data=mydata, pch=16, col = c("red","blue")[as.factor(mydata$Married)])
z4 <- glm(buy ~ College, data=mydata, family="binomial"(link="logit"))
AIC(z4)                             # AIC value
-2*(logLik(z.null) - logLik(z4))    # G value
anova(z.null, z4, test="Chi")       # p value
z5 <- glm(buy ~ College, data=mydata, family="binomial"(link="logit"))
AIC(z5)                             # AIC value
-2*(logLik(z.null) - logLik(z5))    # G value
z5 <- glm(buy ~ Professional, data=mydata, family="binomial"(link="logit"))
AIC(z5)                             # AIC value
-2*(logLik(z.null) - logLik(z5))    # G value
anova(z.null, z5, test="Chi")       # p value
# $ Retired        : Factor w/ 2 levels "0","1": 1 1 1 1 1 1 2 2 1 2 ...
z6 <- glm(buy ~ Retired, data=mydata, family="binomial"(link="logit"))
AIC(z6)                             # AIC value
-2*(logLik(z.null) - logLik(z56)    # G value
anova(z.null, z6, test="Chi")       # p value
z6 <- glm(buy ~ Retired, data=mydata, family="binomial"(link="logit"))
AIC(z6)                             # AIC value
-2*(logLik(z.null) - logLik(z6)    # G value
anova(z.null, z6, test="Chi")       # p value
z6 <- glm(buy ~ Retired, data=mydata, family="binomial"(link="logit"))
AIC(z6)                             # AIC value
-2*(logLik(z.null) - logLik(z6)    # G value
-2*(logLik(z.null) - logLik(z6))    # G value
AIC(z6)                             # AIC value
-2*(logLik(z.null) - logLik(z6))    # G value
anova(z.null, z6, test="Chi")       # p value
z7 <- glm(buy ~ Unemployed, data=mydata, family="binomial"(link="logit"))
AIC(z7)                             # AIC value
-2*(logLik(z.null) - logLik(z7))    # G value
anova(z.null, z7, test="Chi")       # p value
z8 <- glm(buy ~ Dual.Income, data=mydata, family="binomial"(link="logit"))
AIC(z8)                             # AIC value
-2*(logLik(z.null) - logLik(z8))    # G value
anova(z.null, z8, test="Chi")       # p value
z9 <- glm(buy ~ Children, data=mydata, family="binomial"(link="logit"))
AIC(z9)                             # AIC value
-2*(logLik(z.null) - logLik(z9))    # G value
anova(z.null, z9, test="Chi")       # p value
z10 <- glm(buy ~ Prev.Child.Mag, data=mydata, family="binomial"(link="logit"))
AIC(z10)                             # AIC value
-2*(logLik(z.null) - logLik(z10))    # G value
anova(z.null, z10, test="Chi")       # p value
z11 <- glm(buy ~ Prev.Parent.Mag, data=mydata, family="binomial"(link="logit"))
AIC(z11)                             # AIC value
-2*(logLik(z.null) - logLik(z11))    # G value
anova(z.null, z11, test="Chi")       # p value
plot(jitter(buy, f = 0.5)~ income, data=mydata, pch=16, col = c("red","blue")[as.factor(mydata$Married)], main = "Buy Vs. Income", xlab = "Income ($/year)", ylab = "Buy")
plot(jitter(buy, f = 0.5)~ income, data=mydata, pch=16, col = c("red","blue")[as.factor(mydata$Married)], main = "Buy Vs. Income", xlab = "Income ($/year)", ylab = "Buy", legend(2000,9.5))
plot(jitter(buy, f = 0.5)~ income, data=mydata, pch=16, col = c("red","blue")[as.factor(mydata$Married)], main = "Buy Vs. Income", xlab = "Income ($/year)", ylab = "Buy", legend(2000,9.5,c("Married", "Not Married")))
plot(jitter(buy, f = 0.5)~ income, data=mydata, pch=16, col = c("red","blue")[as.factor(mydata$Married)], main = "Buy Vs. Income", xlab = "Income ($/year)", ylab = "Buy")
legend(2000,9.5, c("Married","Not Married"))
legend(2000,9.5, c("Married","Not Married"))
legend(1, 95, legend=c("Married", "Not marrioed"), lty=1:2, cex=0.8)
legend(1, 95, legend=c("Married", "Not marrioed"),
col=c("red", "blue"), lty=1:2, cex=0.8)
legend(1, 95, legend=c("Married", "Not marrioed"),
col=c("red", "blue"), lty=1:2, cex=0.8)
plot(jitter(buy, f = 0.5)~ income, data=mydata, pch=16, col = c("red","blue")[as.factor(mydata$Married)], main = "Buy Vs. Income", xlab = "Income ($/year)", ylab = "Buy")
legend(1, 1, legend=c("Married", "Not marrioed"),
col=c("red", "blue"), lty=1:2, cex=0.8)
legend(1, 1, legend=c("Married", "Not Married"),
col=c("red", "blue"), pch=16:16, cex=0.8)
plot(jitter(buy, f = 0.5)~ income, data=mydata, pch=16, col = c("red","blue")[as.factor(mydata$Married)], main = "Buy Vs. Income", xlab = "Income ($/year)", ylab = "Buy")
legend(1, 1, legend=c("Married", "Not Married"),
col=c("red", "blue"), pch=16:16, cex=0.8)
plot(jitter(buy, f = 0.5)~ income, data=mydata, pch=16, col = c("red","blue")[as.factor(mydata$Married)], main = "Buy Vs. Income", xlab = "Income ($/year)", ylab = "Buy")
legend(1, 1, legend=c("Not Married", "Married"),
col=c("red", "blue"), pch=16:16, cex=0.8)
z12 <- glm(buy ~ income + Married + income*Married, data=mydata, family="binomial"(link="logit"))
AIC(z12)                             # AIC value
-2*(logLik(z.null) - logLik(z12))    # G value
anova(z.null, z12, test="Chi")       # p value
View(mydata)
anova(z12, z1, test="Chi")       # p value
-2*(logLik(12) - logLik(z1))    # G value
anova(z12, z1, test="Chi")       # p value
-2*(logLik(z12) - logLik(z1))    # G value
-2*(logLik(z1) - logLik(z12))    # G value
-2*(logLik(z.null) - logLik(z12))    # G value
anova(z12, z1, test="Chi")       # p value
-2*(logLik(z1) - logLik(z12))    # G value
anova(z12, z2, test="Chi")       # p value
-2*(logLik(z2) - logLik(z12))    # G value
# Model 3
z13 <- glm(buy ~ income + Married, data=mydata, family="binomial"(link="logit"))
# Testing the Significance of income [i.e. Compare (3) and (4)]
anova(z12, z3, test="Chi")       # p value
-2*(logLik(z3) - logLik(z12))    # G value
z14 <- glm(buy ~ income + Married + Unemployed + income*Married + income*Unemployed, data=mydata, family="binomial"(link="logit"))
str(mydata)
# Testing the Significance of Unemployed [i.e. Compare (3) and (4)]
anova(z14, z12, test="Chi")       # p value
-2*(logLik(z12) - logLik(z14))    # G value
# Model C
z15 <- glm(buy ~ income + Married + Dual.Income + income*Married + income*Dual.Income, data=mydata, family="binomial"(link="logit"))
anova(z15, z12, test="Chi")       # p value
-2*(logLik(z12) - logLik(z15))    # G value
z16 <- glm(buy ~ income + Married + Professional + income*Married + income*Professional, data=mydata, family="binomial"(link="logit"))
anova(z16, z12, test="Chi")       # p value
-2*(logLik(z12) - logLik(z16))    # G value
anova(z17, z12, test="Chi")       # p value
-2*(logLik(z12) - logLik(z17))    # G value
z17 <- glm(buy ~ income + Married + College + income*Married + income*College, data=mydata, family="binomial"(link="logit"))
anova(z17, z12, test="Chi")       # p value
-2*(logLik(z12) - logLik(z17))    # G value
z18 <- glm(buy ~ income + Married + Prev.Child.Mag + income*Married + income*Prev.Child.Magazine, data=mydata, family="binomial"(link="logit"))
anova(z18, z12, test="Chi")       # p value
-2*(logLik(z12) - logLik(z18))    # G value
z18 <- glm(buy ~ income + Married + Prev.Child.Mag + income*Married + income*Prev.Child.Mag, data=mydata, family="binomial"(link="logit"))
anova(z18, z12, test="Chi")       # p value
-2*(logLik(z12) - logLik(z18))    # G value
z19 <- glm(buy ~ income + Married + Female + income*Married + income*Female, data=mydata, family="binomial"(link="logit"))
anova(z19, z12, test="Chi")       # p value
-2*(logLik(z12) - logLik(z19))    # G value
# Model H
z20 <- glm(buy ~ income + Married + Female + income*Married + income*Female, data=mydata, family="binomial"(link="logit"))
anova(z19, z20, test="Chi")       # p value
-2*(logLik(z20) - logLik(z19))    # G value
z20 <- glm(buy ~ income + Married + Female + income*Married, data=mydata, family="binomial"(link="logit"))
anova(z19, z20, test="Chi")       # p value
-2*(logLik(z20) - logLik(z19))    # G value
1 - (logLik(z.null)/logLik(z20))^(2/nrow(mydata))
#Scaled R2
(1 - (logLik(z.null)/logLik(z1))^(2/nrow(mydata)))/(1-logLik(z.null)^(2/nrow(mydata)))
(1 - (logLik(z.null)/logLik(z1))^(2/nrow(mydata)))/(1 - (-13.86294^(2/nrow(mydata))))
logLik(z.null)
#Scaled R2
(1 - (logLik(z.null)/logLik(z20))^(2/nrow(mydata)))/(1-logLik(z.null)^(2/nrow(mydata)))
(1 - (logLik(z.null)/logLik(z20))^(2/nrow(mydata)))/(1 - (-323.0265^(2/nrow(mydata))))
AIC(z20)     # takes into account number of variables (penalized if more)
AICc(z20)    # takes into account number of variables and sample size
library(AICcmodavg)
install.packages("AICcmodavg")
AICc(z20)
library(AICcmodavg)
# AIC, smaller (more negative) is better
AIC(z20)     # takes into account number of variables (penalized if more)
# [1] 222.4329
AICc(z20)    # takes into account number of vari
# read in the csv file
mydatax <- read.csv("2015-11-04_purhcase_data.csv", header=TRUE)
# Create an empty dataframe that you will fill with
df <- data.frame(matrix(ncol = 9, nrow = 51))
colnames(df) <- c("correct.event", "correct.non.event", "incorrect.event", "incorrect.non.event", "correct.percent", "sensitivity", "specificity", "false.pos", "false.neg")
df
prob.level <- seq(0, 1, length.out=51) # create a vector with different possible probabilities
prob.level
class.table.data <- cbind(prob.level, df) # combine your vector of probabilities and your empty dataframe
class.table.data # Your dataframe has one row for each probability cut-off
# fill empty cells in your dataframe with 0
class.table.data$correct.non.event <- rep(c(0), c(51))
class.table.data$correct.event <- rep(c(0), c(51))
class.table.data$incorrect.non.event <- rep(c(0), c(51))
class.table.data$incorrect.event <- rep(c(0), c(51))
class.table.data
# This loop will try out the different probability cut-off values and fill in how many correct and incorrect events and non-events you have based on your data.
for (i in 1:51) {
class.table <- table(mydata$buy, fitted(z20) > class.table.data$prob.level[i])
col.true.num <- grep("TRUE", colnames(class.table))
col.false.num <- grep("FALSE", colnames(class.table))
if (length(col.true.num) > 0) {
class.table.data$incorrect.non.event [i] <- class.table[1, col.true.num]
class.table.data$correct.event [i] <- class.table[2, col.true.num] }
if (length(col.false.num) > 0) {
class.table.data$correct.non.event [i] <- class.table[1, col.false.num]
class.table.data$incorrect.event [i] <- class.table[2, col.false.num] }  }
class.table.data
# You will use this information to fill in the rest of your classification table.
class.table.data$correct.percent <- (class.table.data$correct.event + class.table.data$correct.non.event)/nrow(mydata)
class.table.data$sensitivity <- (class.table.data$correct.event)/nrow(mydata)
class.table.data$specificity <- (class.table.data$correct.non.event)/nrow(mydata)
class.table.data$false.neg <- (class.table.data$incorrect.non.event)/nrow(mydata)
class.table.data$false.pos <- (class.table.data$incorrect.event)/nrow(mydata)
class.table.data
# look at the column percent correct
# Loss of error when talking about errors
class.table.data
write.csv(class.table.data, file = "ClassTable.csv")
# Create an empty dataframe that you will fill with
df <- data.frame(matrix(ncol = 9, nrow = 100))
colnames(df) <- c("correct.event", "correct.non.event", "incorrect.event", "incorrect.non.event", "correct.percent", "sensitivity", "specificity", "false.pos", "false.neg")
df
prob.level <- seq(0, 1, length.out=51) # create a vector with different possible probabilities
prob.level
class.table.data <- cbind(prob.level, df) # combine your vector of probabilities and your empty dataframe
class.table.data # Your dataframe has one row for each probability cut-off
# fill empty cells in your dataframe with 0
class.table.data$correct.non.event <- rep(c(0), c(51))
class.table.data$correct.event <- rep(c(0), c(51))
class.table.data$incorrect.non.event <- rep(c(0), c(51))
class.table.data$incorrect.event <- rep(c(0), c(51))
class.table.data
# This loop will try out the different probability cut-off values and fill in how many correct and incorrect events and non-events you have based on your data.
for (i in 1:51) {
class.table <- table(mydata$buy, fitted(z20) > class.table.data$prob.level[i])
col.true.num <- grep("TRUE", colnames(class.table))
col.false.num <- grep("FALSE", colnames(class.table))
if (length(col.true.num) > 0) {
class.table.data$incorrect.non.event [i] <- class.table[1, col.true.num]
class.table.data$correct.event [i] <- class.table[2, col.true.num] }
if (length(col.false.num) > 0) {
class.table.data$correct.non.event [i] <- class.table[1, col.false.num]
class.table.data$incorrect.event [i] <- class.table[2, col.false.num] }  }
class.table.data
# You will use this information to fill in the rest of your classification table.
class.table.data$correct.percent <- (class.table.data$correct.event + class.table.data$correct.non.event)/nrow(mydata)
class.table.data$sensitivity <- (class.table.data$correct.event)/nrow(mydata)
class.table.data$specificity <- (class.table.data$correct.non.event)/nrow(mydata)
class.table.data$false.neg <- (class.table.data$incorrect.non.event)/nrow(mydata)
class.table.data$false.pos <- (class.table.data$incorrect.event)/nrow(mydata)
class.table.data
write.csv(class.table.data, file = "ClassTable.csv")
str(mydata)
View(mydata)
str(mydata)
z14 <- glm(buy ~ income + Married + Professional + income*Married + income*Professional, data=mydata, family="binomial"(link="logit"))
# Testing the Significance of Unemployed [i.e. Compare (3) and (4)]
anova(z14, z12, test="Chi")       # p value
-2*(logLik(z12) - logLik(z14))    # G value
# Model D
z16 <- glm(buy ~ income + Married + Unemployed + income*Married + income*Unemployed, data=mydata, family="binomial"(link="logit"))
anova(z16, z12, test="Chi")       # p value
-2*(logLik(z12) - logLik(z16))    # G value
# Pseudo R2
1 - (logLik(z.null)/logLik(z12))^(2/nrow(mydata))
(1 - (logLik(z.null)/logLik(z12))^(2/nrow(mydata)))/(1-logLik(z.null)^(2/nrow(mydata)))
(1 - (logLik(z.null)/logLik(z12))^(2/nrow(mydata)))/(1 - (-323.0265^(2/nrow(mydata))))
logLik(z.null)
# 'log Lik.' -323.0265 (df=1)
AIC(z12)
AICc(z12)
library(AICcmodavg)
AICc(z12)
AIC(z12)
AICc(z12)
### 27.	Create a classification table for this data based on the final model. Include the full classification table in your output. Remember that you can output dataframes to a .csv file using write.csv().
# This will try out many different cut-off points to give you an idea of how to maximize or minimize different values.
# For example, you might want to maximize percentage of correct predictions.
# For example, you might want to minimize false negatives.
# Create an empty dataframe that you will fill with
df <- data.frame(matrix(ncol = 9, nrow = 51))
colnames(df) <- c("correct.event", "correct.non.event", "incorrect.event", "incorrect.non.event", "correct.percent", "sensitivity", "specificity", "false.pos", "false.neg")
df
prob.level <- seq(0, 1, length.out=51) # create a vector with different possible probabilities
prob.level
class.table.data <- cbind(prob.level, df) # combine your vector of probabilities and your empty dataframe
class.table.data # Your dataframe has one row for each probability cut-off
# fill empty cells in your dataframe with 0
class.table.data$correct.non.event <- rep(c(0), c(51))
class.table.data$correct.event <- rep(c(0), c(51))
class.table.data$incorrect.non.event <- rep(c(0), c(51))
class.table.data$incorrect.event <- rep(c(0), c(51))
class.table.data
# This loop will try out the different probability cut-off values and fill in how many correct and incorrect events and non-events you have based on your data.
for (i in 1:51) {
class.table <- table(mydata$buy, fitted(z12) > class.table.data$prob.level[i])
col.true.num <- grep("TRUE", colnames(class.table))
col.false.num <- grep("FALSE", colnames(class.table))
if (length(col.true.num) > 0) {
class.table.data$incorrect.non.event [i] <- class.table[1, col.true.num]
class.table.data$correct.event [i] <- class.table[2, col.true.num] }
if (length(col.false.num) > 0) {
class.table.data$correct.non.event [i] <- class.table[1, col.false.num]
class.table.data$incorrect.event [i] <- class.table[2, col.false.num] }  }
class.table.data
# You will use this information to fill in the rest of your classification table.
class.table.data$correct.percent <- (class.table.data$correct.event + class.table.data$correct.non.event)/nrow(mydata)
class.table.data$sensitivity <- (class.table.data$correct.event)/nrow(mydata)
class.table.data$specificity <- (class.table.data$correct.non.event)/nrow(mydata)
class.table.data$false.neg <- (class.table.data$incorrect.non.event)/nrow(mydata)
class.table.data$false.pos <- (class.table.data$incorrect.event)/nrow(mydata)
class.table.data
write.csv(class.table.data, file = "ClassTable.csv")
(1 - (logLik(z.null)/logLik(z12))^(2/nrow(mydata)))/(1-(-323.0265)^(2/nrow(mydata)))
logLik(z12)
(1 - ((-323.0265))/(-112.8367))^(2/nrow(mydata)))/(1-(-323.0265)^(2/nrow(mydata)))
(1 - ((-323.0265))/(-112.8367)^(2/nrow(mydata)))/(1-(-323.0265)^(2/nrow(mydata)))
nrow(mydata)
(1 - ((-323.0265))/(-112.8367)^(2/(673)))/(1-(-323.0265)^(2/673))
(-323.0265)^(2/673)
2/673
(-323.0265)^(0.002971768)
mean(mydata$buy)
expected variance = 0.1857355*(1-0.1857355)
expected_variance = 0.1857355*(1-0.1857355)
expected_varance
expected variance
expected_variance
expected_variance = 0.1857355*(1-0.1857355)
expected_variance
var(mydata$buy)
summary(mydata)
summary(mydata$buy)
summary(z.null)
